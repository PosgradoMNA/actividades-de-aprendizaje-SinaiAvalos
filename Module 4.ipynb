{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/actividades-de-aprendizaje-SinaiAvalos/blob/main/Module%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Sinaì Avalos Rivera A01730466*\n",
        "\n",
        "\n",
        "# **MODEL DEVELOPMENT**\n",
        "\n",
        "A model or estimator can be thought of as a mathematical equation used to predict a\n",
        "value given one or more other values. Relating one or more independent variables or features to dependent variables.\n",
        "\n",
        "\n",
        "**LINEAR AND MULTIPLE LINEAR REGRESSION**\n",
        "\n",
        "Linear Regression will refer to one independent variable to make a prediction.\n",
        "\n",
        "\n",
        "Multiple Linear Regression will refer to multiple independent variables to make a prediction.\n",
        "\n",
        "\n",
        "Simple Linear Regression (or SLR) is: A method to help us understand the relationship between\n",
        "two variables: The predictor (independent) variable x, and the target (dependent) variable y (Y=B0+B1X)\n",
        "\n",
        "\n",
        "We then use these training points to fit our model; the results of the training points\n",
        "are the parameters. We usually store the data points in two dataframe\n",
        "or numpy arrays. The value we would like to predict is called\n",
        "the target that we store in the array y, we store the dependent variable in the dataframe\n",
        "or array X. Each sample corresponds to a different row.\n",
        "\n",
        "\n",
        "\n",
        "- We have a set of training points - We use these training points to fit or train\n",
        "the model and get parameters - We then use these parameters in the model\n",
        "\n",
        "- We can use this model to predict values that we haven't seen.\n",
        "\n",
        "- We can comparE the predicted value to the actual value.\n",
        "\n",
        "To fit the model in Python:\n",
        "\n",
        "* We import linear model from scikit-learn\n",
        "\n",
        "* Create a Linear Regression Object using the constructor. \n",
        "\n",
        "* Define the predictor variable and target\n",
        "variable. \n",
        "\n",
        "*Then use the method fit to fit the model and\n",
        "find the parameters b0 and b1. \n",
        "\n",
        "* The input are the features and the targets.\n",
        "We can obtain a prediction using the method predict.\n",
        "\n",
        "\n",
        "```\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lm=LinearRegression()\n",
        "X=df[['highwway-mpg']]\n",
        "Y=df[['price']]\n",
        "lm.fit(X,Y)\n",
        "Yhat = lm.predict(X)\n",
        "\n",
        "\n",
        "lm.intercept_\n",
        "\n",
        "lm.coef_\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Multiple Linear Regression**\n",
        "\n",
        "It iss used to explain the relationship between\n",
        "One continuous target (Y) variable, and Two or more predictor (X) variables.\n",
        "\n",
        "We can fit the Multiple linear regression as follows:\n",
        "\n",
        "\n",
        "* We can extract the 4 predictor variables and store them in the variable Z.\n",
        "* Then train the model as before using the method train, with the features or dependent variables and the targets \n",
        "* We can also obtain a prediction using the\n",
        "method predict. \n",
        "* The output is an array with the same number of elements as number of samples.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  Z = df[['horsepower', 'curb-weight', 'engine-size', higway-mpg']]\n",
        "\n",
        "lm.fit(Z,df['price'])\n",
        "\n",
        "Yhat = lm.predict(X)\n",
        "\n",
        "\n",
        "lm.intercept_\n",
        "\n",
        "lm.coef_ #array \n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**MODEL EVALUAITON USING VISUALIZATION**\n",
        "\n",
        "**Regresion Plot**\n",
        "\n",
        "The relationship between two variables,\n",
        "The strength of the correlation, and\n",
        "The direction of the relationship (positive or negative).\n",
        "The horizontal axis is the independent variable.\n",
        "The vertical axis is the dependent variable.\n",
        "Each point represents a different target point.\n",
        "The fitted line represents the predicted value.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  import seaborn as sns\n",
        "\n",
        "sns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\n",
        "plt.ylim(0,)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Residual Plot**\n",
        "\n",
        "The residual plot represents the error between the actual values.\n",
        "Examining the predicted value and actual value we see a difference.We obtain that value by subtracting the predicted value and the actual target value. \n",
        "\n",
        "We then plot that value on the vertical axis, with the dependent variable as the horizontal\n",
        "axis.\n",
        "\n",
        "Looking at the plot gives us some insight into our data.\n",
        "We expect to see the results to have zero mean. Distributed evenly around the x axis with similar variance; there is no curvature.\n",
        "\n",
        "\n",
        "The residuals that are not randomly separated; this suggests the linear assumption is incorrect.\n",
        "\n",
        "```\n",
        "#  import seaborn as sns\n",
        "\n",
        "sns.residplot(df[\"highway-mpg\"], df[\"price\"])\n",
        "plt.ylim(0,)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Distribution Plots**\n",
        "\n",
        "\n",
        "A distribution plot counts the predicted value versus the actual value.\n",
        "These plots are extremely useful for visualizing models with more than one independent variable\n",
        "or feature.\n",
        "\n",
        "\n",
        "Let's look at a simplified example: - We examine the vertical axis.\n",
        "- We then count and plot the number of predicted points that are approximately equal to one.\n",
        "- We then count and plot the number of predicted points that are approximately equal to two.\n",
        "- We repeat the process for predicted points that are approximately equal to three.\n",
        "- Then we repeat the process for the target values.\n",
        "- The values of the targets and predicted values are continuous.\n",
        "A histogram is for discrete values.\n",
        "Therefore pandas will convert them to a distribution.\n",
        "The vertical access is scaled to make the area under the distribution equal to one.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  import seaborn as sns\n",
        "\n",
        "axl ? sns.distplot(df['price'], hist=False, color=\"r\", label\"Actual Value\")\n",
        "\n",
        "sns.distplot(Yhat, hist=False, color=\"b\", label=\"Fitted Values\", ax=ax1)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**POLYNOMIAL REGRESSION AND PIPELINES**\n",
        "\n",
        "What do we do when a linear model is not the best fit for our data?\n",
        "\n",
        "Another type of regression model: The polynomial regression\n",
        "\n",
        "We Transform our data into a polynomial, then use linear regression to fit the parameter.\n",
        "\n",
        "\n",
        "\n",
        "Pipelines are a way to simplify your code.\n",
        "Polynomial regression is a special case of the general linear regression. This method is beneficial for describing curvilinear relationships.\n",
        "\n",
        "\n",
        "In all cases, the relationship between the variable and the parameter is always linear.\n",
        "\n",
        "\n",
        "- In Python, we do this by using the polyfit() function.\n",
        "- In this example, we develop a third order polynomial regression model base.\n",
        "- We can print out the model.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  f=np.polyfit(x,y,3)\n",
        "\n",
        "p = np.polyld(f)\n",
        "\n",
        "print(p)\n",
        "```\n",
        "\n",
        "We can also have multi-dimensional polynomial linear regression. The expression can get complicated.\n",
        "\n",
        "- Numpy’s “polyfit” function cannot perform this type of regression.\n",
        "- We use the \"preprocessing\" library in sci-kitlearn, to create a polynomial feature object.\n",
        "- The constructor takes the degree of the polynomial as a parameter.\n",
        "- Then we transform the features into a polynomial feature with the “fit_transform” method.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "pr=PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "x_poly=pr.fit_transform(x(['horsepower','curb-weight']))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "-Applying the method, we transform the data We now have a new set of features that are\n",
        "a transformed version of our original features.\n",
        "As the dimension of the data gets larger we may want to normalize multiple features in\n",
        "scikit-learn, instead, we can use the preprocessing module to simplify many tasks.\n",
        "\n",
        "-For example, we can Standardize each feature simultaneously.\n",
        "- We import “StandardScaler”.\n",
        "- We train the object, fit the scale object;\n",
        "then transform the data into a new dataframe on array “x_scale”.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  from sklearn.preprocessing import StandarScaler\n",
        "SCALE=StandardScaler()\n",
        "\n",
        "SCALE.fit(x_data[['horsepower', 'highway-mpg']])\n",
        "\n",
        "x_scale=SCALE.transform(x_data[['horsepower', 'highway-mpg']])\n",
        "\n",
        "```\n",
        "\n",
        "There are more normalization methods available in the preprocessing library, as well as other\n",
        "transformations.\n",
        "\n",
        "\n",
        "We can simplify our code by using a pipeline library.\n",
        "There are many steps to getting a prediction, for example, Normalization, Polynomial transform,\n",
        "and Linear regression.\n",
        "\n",
        "\n",
        "Pipelines sequentially perform a series of transformation.\n",
        "The last step carries out a prediction.\n",
        "- First we import all the modules we need.\n",
        "- Then we import the library Pipeline.\n",
        "- We create a list of tuples, the first element in the tuple contains the name of the estimator:\n",
        "model.\n",
        "- The second element contain model constructor.\n",
        "- We input the list in the pipeline constructor.\n",
        "- We now have a pipeline object. We can train the pipeline by applying the train method to the Pipeline object. We can also produce a prediction as well.\n",
        "\n",
        "```\n",
        "#  \n",
        "```"
      ],
      "metadata": {
        "id": "HdANV7r0_1yR"
      }
    }
  ]
}